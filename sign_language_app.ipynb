{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 397/397 [00:01<00:00, 216.70it/s]\n",
      "100%|██████████| 409/409 [00:01<00:00, 224.84it/s]\n",
      "100%|██████████| 399/399 [00:01<00:00, 250.62it/s]\n",
      "100%|██████████| 420/420 [00:01<00:00, 214.85it/s]\n",
      "100%|██████████| 431/431 [00:01<00:00, 254.94it/s]\n",
      "100%|██████████| 422/422 [00:01<00:00, 216.77it/s]\n",
      "100%|██████████| 440/440 [00:01<00:00, 312.68it/s]\n",
      "100%|██████████| 477/477 [00:01<00:00, 296.76it/s]\n",
      "100%|██████████| 481/481 [00:01<00:00, 251.32it/s]\n",
      "100%|██████████| 487/487 [00:01<00:00, 262.80it/s]\n",
      "100%|██████████| 467/467 [00:01<00:00, 263.25it/s]\n",
      "100%|██████████| 439/439 [00:02<00:00, 197.30it/s]\n",
      "100%|██████████| 471/471 [00:01<00:00, 277.93it/s]\n",
      "100%|██████████| 500/500 [00:02<00:00, 196.77it/s]\n",
      "100%|██████████| 482/482 [00:01<00:00, 245.91it/s]\n",
      "100%|██████████| 469/469 [00:01<00:00, 292.13it/s]\n",
      "100%|██████████| 453/453 [00:01<00:00, 245.25it/s]\n",
      "100%|██████████| 479/479 [00:02<00:00, 205.12it/s]\n",
      "100%|██████████| 482/482 [00:01<00:00, 242.48it/s]\n",
      "100%|██████████| 481/481 [00:02<00:00, 222.29it/s]\n",
      "100%|██████████| 472/472 [00:02<00:00, 223.85it/s]\n",
      "100%|██████████| 448/448 [00:01<00:00, 246.84it/s]\n",
      "100%|██████████| 465/465 [00:02<00:00, 211.66it/s]\n",
      "100%|██████████| 451/451 [00:01<00:00, 261.73it/s]\n",
      "100%|██████████| 469/469 [00:01<00:00, 235.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 3, 3, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,050,852\n",
      "Trainable params: 4,008,829\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "303/303 [==============================] - 373s 1s/step - loss: 3.1498 - mae: 3.1498 - val_loss: 1.6492 - val_mae: 1.6492 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "303/303 [==============================] - 371s 1s/step - loss: 1.2597 - mae: 1.2597 - val_loss: 1.6574 - val_mae: 1.6574 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "303/303 [==============================] - 339s 1s/step - loss: 0.8770 - mae: 0.8770 - val_loss: 0.6630 - val_mae: 0.6630 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "303/303 [==============================] - 342s 1s/step - loss: 0.7172 - mae: 0.7172 - val_loss: 0.4759 - val_mae: 0.4759 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "303/303 [==============================] - 348s 1s/step - loss: 0.6217 - mae: 0.6217 - val_loss: 0.5747 - val_mae: 0.5747 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "303/303 [==============================] - 334s 1s/step - loss: 0.5991 - mae: 0.5991 - val_loss: 0.9418 - val_mae: 0.9418 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "303/303 [==============================] - 336s 1s/step - loss: 0.6188 - mae: 0.6188 - val_loss: 0.7040 - val_mae: 0.7040 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "303/303 [==============================] - 336s 1s/step - loss: 0.5673 - mae: 0.5673 - val_loss: 0.2732 - val_mae: 0.2732 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "303/303 [==============================] - 339s 1s/step - loss: 0.4838 - mae: 0.4838 - val_loss: 0.3545 - val_mae: 0.3545 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "303/303 [==============================] - 358s 1s/step - loss: 0.5481 - mae: 0.5481 - val_loss: 0.3524 - val_mae: 0.3524 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "303/303 [==============================] - 338s 1s/step - loss: 0.4601 - mae: 0.4601 - val_loss: 0.2513 - val_mae: 0.2513 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "303/303 [==============================] - 341s 1s/step - loss: 0.4417 - mae: 0.4417 - val_loss: 0.4694 - val_mae: 0.4694 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "303/303 [==============================] - 338s 1s/step - loss: 0.5388 - mae: 0.5388 - val_loss: 0.4701 - val_mae: 0.4701 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "303/303 [==============================] - 335s 1s/step - loss: 0.4448 - mae: 0.4448 - val_loss: 0.4339 - val_mae: 0.4339 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "303/303 [==============================] - 348s 1s/step - loss: 0.4475 - mae: 0.4475 - val_loss: 0.5532 - val_mae: 0.5532 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.4359 - mae: 0.4359\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "303/303 [==============================] - 353s 1s/step - loss: 0.4359 - mae: 0.4359 - val_loss: 0.3887 - val_mae: 0.3887 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "303/303 [==============================] - 406s 1s/step - loss: 0.4109 - mae: 0.4109 - val_loss: 0.2691 - val_mae: 0.2691 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "303/303 [==============================] - 328s 1s/step - loss: 0.4099 - mae: 0.4099 - val_loss: 0.3936 - val_mae: 0.3936 - lr: 9.0000e-04\n",
      "Epoch 19/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.3680 - mae: 0.3680 - val_loss: 0.3177 - val_mae: 0.3177 - lr: 9.0000e-04\n",
      "Epoch 20/100\n",
      "303/303 [==============================] - 345s 1s/step - loss: 0.4314 - mae: 0.4314 - val_loss: 0.7954 - val_mae: 0.7954 - lr: 9.0000e-04\n",
      "Epoch 21/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.4933 - mae: 0.4933\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "303/303 [==============================] - 327s 1s/step - loss: 0.4933 - mae: 0.4933 - val_loss: 0.5092 - val_mae: 0.5092 - lr: 9.0000e-04\n",
      "Epoch 22/100\n",
      "303/303 [==============================] - 326s 1s/step - loss: 0.3787 - mae: 0.3787 - val_loss: 0.4438 - val_mae: 0.4438 - lr: 8.1000e-04\n",
      "Epoch 23/100\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.3531 - mae: 0.3531 - val_loss: 0.1949 - val_mae: 0.1949 - lr: 8.1000e-04\n",
      "Epoch 24/100\n",
      "303/303 [==============================] - 326s 1s/step - loss: 0.3406 - mae: 0.3406 - val_loss: 0.2614 - val_mae: 0.2614 - lr: 8.1000e-04\n",
      "Epoch 25/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.2231 - val_mae: 0.2231 - lr: 8.1000e-04\n",
      "Epoch 26/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.3326 - mae: 0.3326 - val_loss: 0.2903 - val_mae: 0.2903 - lr: 8.1000e-04\n",
      "Epoch 27/100\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.3153 - mae: 0.3153 - val_loss: 0.2612 - val_mae: 0.2612 - lr: 8.1000e-04\n",
      "Epoch 28/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.3340 - mae: 0.3340\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "303/303 [==============================] - 325s 1s/step - loss: 0.3340 - mae: 0.3340 - val_loss: 0.3297 - val_mae: 0.3297 - lr: 8.1000e-04\n",
      "Epoch 29/100\n",
      "303/303 [==============================] - 327s 1s/step - loss: 0.3076 - mae: 0.3076 - val_loss: 0.2359 - val_mae: 0.2359 - lr: 7.2900e-04\n",
      "Epoch 30/100\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.3770 - mae: 0.3770 - val_loss: 0.2572 - val_mae: 0.2572 - lr: 7.2900e-04\n",
      "Epoch 31/100\n",
      "303/303 [==============================] - 325s 1s/step - loss: 0.3434 - mae: 0.3434 - val_loss: 0.2510 - val_mae: 0.2510 - lr: 7.2900e-04\n",
      "Epoch 32/100\n",
      "303/303 [==============================] - 359s 1s/step - loss: 0.3562 - mae: 0.3562 - val_loss: 0.2063 - val_mae: 0.2063 - lr: 7.2900e-04\n",
      "Epoch 33/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.3210 - mae: 0.3210\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.3210 - mae: 0.3210 - val_loss: 0.2504 - val_mae: 0.2504 - lr: 7.2900e-04\n",
      "Epoch 34/100\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.3062 - mae: 0.3062 - val_loss: 0.2354 - val_mae: 0.2354 - lr: 6.5610e-04\n",
      "Epoch 35/100\n",
      "303/303 [==============================] - 351s 1s/step - loss: 0.2880 - mae: 0.2880 - val_loss: 0.2723 - val_mae: 0.2723 - lr: 6.5610e-04\n",
      "Epoch 36/100\n",
      "303/303 [==============================] - 340s 1s/step - loss: 0.2928 - mae: 0.2928 - val_loss: 0.2155 - val_mae: 0.2155 - lr: 6.5610e-04\n",
      "Epoch 37/100\n",
      "303/303 [==============================] - 331s 1s/step - loss: 0.3285 - mae: 0.3285 - val_loss: 0.2189 - val_mae: 0.2189 - lr: 6.5610e-04\n",
      "Epoch 38/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2989 - mae: 0.2989\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.2989 - mae: 0.2989 - val_loss: 0.2243 - val_mae: 0.2243 - lr: 6.5610e-04\n",
      "Epoch 39/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.2776 - mae: 0.2776 - val_loss: 0.1311 - val_mae: 0.1311 - lr: 5.9049e-04\n",
      "Epoch 40/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.2827 - mae: 0.2827 - val_loss: 0.1895 - val_mae: 0.1895 - lr: 5.9049e-04\n",
      "Epoch 41/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.2962 - mae: 0.2962 - val_loss: 0.3091 - val_mae: 0.3091 - lr: 5.9049e-04\n",
      "Epoch 42/100\n",
      "303/303 [==============================] - 339s 1s/step - loss: 0.2896 - mae: 0.2896 - val_loss: 0.2113 - val_mae: 0.2113 - lr: 5.9049e-04\n",
      "Epoch 43/100\n",
      "303/303 [==============================] - 325s 1s/step - loss: 0.2780 - mae: 0.2780 - val_loss: 0.1679 - val_mae: 0.1679 - lr: 5.9049e-04\n",
      "Epoch 44/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2715 - mae: 0.2715\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2715 - mae: 0.2715 - val_loss: 0.1743 - val_mae: 0.1743 - lr: 5.9049e-04\n",
      "Epoch 45/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.2633 - mae: 0.2633 - val_loss: 0.1455 - val_mae: 0.1455 - lr: 5.3144e-04\n",
      "Epoch 46/100\n",
      "303/303 [==============================] - 321s 1s/step - loss: 0.2710 - mae: 0.2710 - val_loss: 0.2888 - val_mae: 0.2888 - lr: 5.3144e-04\n",
      "Epoch 47/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2745 - mae: 0.2745 - val_loss: 0.1269 - val_mae: 0.1269 - lr: 5.3144e-04\n",
      "Epoch 48/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2610 - mae: 0.2610 - val_loss: 0.1359 - val_mae: 0.1359 - lr: 5.3144e-04\n",
      "Epoch 49/100\n",
      "303/303 [==============================] - 321s 1s/step - loss: 0.2666 - mae: 0.2666 - val_loss: 0.1548 - val_mae: 0.1548 - lr: 5.3144e-04\n",
      "Epoch 50/100\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.2590 - mae: 0.2590 - val_loss: 0.1429 - val_mae: 0.1429 - lr: 5.3144e-04\n",
      "Epoch 51/100\n",
      "303/303 [==============================] - 328s 1s/step - loss: 0.2555 - mae: 0.2555 - val_loss: 0.1565 - val_mae: 0.1565 - lr: 5.3144e-04\n",
      "Epoch 52/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2649 - mae: 0.2649\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "303/303 [==============================] - 321s 1s/step - loss: 0.2649 - mae: 0.2649 - val_loss: 0.5119 - val_mae: 0.5119 - lr: 5.3144e-04\n",
      "Epoch 53/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.3031 - mae: 0.3031 - val_loss: 0.1678 - val_mae: 0.1678 - lr: 4.7830e-04\n",
      "Epoch 54/100\n",
      "303/303 [==============================] - 336s 1s/step - loss: 0.2600 - mae: 0.2600 - val_loss: 0.1422 - val_mae: 0.1422 - lr: 4.7830e-04\n",
      "Epoch 55/100\n",
      "303/303 [==============================] - 326s 1s/step - loss: 0.2675 - mae: 0.2675 - val_loss: 0.1428 - val_mae: 0.1428 - lr: 4.7830e-04\n",
      "Epoch 56/100\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.2677 - mae: 0.2677 - val_loss: 0.4381 - val_mae: 0.4381 - lr: 4.7830e-04\n",
      "Epoch 57/100\n",
      "303/303 [==============================] - 328s 1s/step - loss: 0.2453 - mae: 0.2453 - val_loss: 0.1158 - val_mae: 0.1158 - lr: 4.7830e-04\n",
      "Epoch 58/100\n",
      "303/303 [==============================] - 326s 1s/step - loss: 0.2482 - mae: 0.2482 - val_loss: 0.1367 - val_mae: 0.1367 - lr: 4.7830e-04\n",
      "Epoch 59/100\n",
      "303/303 [==============================] - 328s 1s/step - loss: 0.2511 - mae: 0.2511 - val_loss: 0.1357 - val_mae: 0.1357 - lr: 4.7830e-04\n",
      "Epoch 60/100\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.2546 - mae: 0.2546 - val_loss: 0.1497 - val_mae: 0.1497 - lr: 4.7830e-04\n",
      "Epoch 61/100\n",
      "303/303 [==============================] - 333s 1s/step - loss: 0.2442 - mae: 0.2442 - val_loss: 0.1224 - val_mae: 0.1224 - lr: 4.7830e-04\n",
      "Epoch 62/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2441 - mae: 0.2441\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "303/303 [==============================] - 327s 1s/step - loss: 0.2441 - mae: 0.2441 - val_loss: 0.1762 - val_mae: 0.1762 - lr: 4.7830e-04\n",
      "Epoch 63/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.2402 - mae: 0.2402 - val_loss: 0.1288 - val_mae: 0.1288 - lr: 4.3047e-04\n",
      "Epoch 64/100\n",
      "303/303 [==============================] - 370s 1s/step - loss: 0.2354 - mae: 0.2354 - val_loss: 0.1474 - val_mae: 0.1474 - lr: 4.3047e-04\n",
      "Epoch 65/100\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.2350 - mae: 0.2350 - val_loss: 0.1878 - val_mae: 0.1878 - lr: 4.3047e-04\n",
      "Epoch 66/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2480 - mae: 0.2480 - val_loss: 0.1310 - val_mae: 0.1310 - lr: 4.3047e-04\n",
      "Epoch 67/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2451 - mae: 0.2451\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "303/303 [==============================] - 328s 1s/step - loss: 0.2451 - mae: 0.2451 - val_loss: 0.1462 - val_mae: 0.1462 - lr: 4.3047e-04\n",
      "Epoch 68/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2422 - mae: 0.2422 - val_loss: 0.1947 - val_mae: 0.1947 - lr: 3.8742e-04\n",
      "Epoch 69/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2346 - mae: 0.2346 - val_loss: 0.1675 - val_mae: 0.1675 - lr: 3.8742e-04\n",
      "Epoch 70/100\n",
      "303/303 [==============================] - 334s 1s/step - loss: 0.2299 - mae: 0.2299 - val_loss: 0.1455 - val_mae: 0.1455 - lr: 3.8742e-04\n",
      "Epoch 71/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2480 - mae: 0.2480 - val_loss: 0.1571 - val_mae: 0.1571 - lr: 3.8742e-04\n",
      "Epoch 72/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2374 - mae: 0.2374\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "303/303 [==============================] - 323s 1s/step - loss: 0.2374 - mae: 0.2374 - val_loss: 0.2247 - val_mae: 0.2247 - lr: 3.8742e-04\n",
      "Epoch 73/100\n",
      "303/303 [==============================] - 327s 1s/step - loss: 0.2306 - mae: 0.2306 - val_loss: 0.1092 - val_mae: 0.1092 - lr: 3.4868e-04\n",
      "Epoch 74/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2240 - mae: 0.2240 - val_loss: 0.2191 - val_mae: 0.2191 - lr: 3.4868e-04\n",
      "Epoch 75/100\n",
      "303/303 [==============================] - 326s 1s/step - loss: 0.2252 - mae: 0.2252 - val_loss: 0.1306 - val_mae: 0.1306 - lr: 3.4868e-04\n",
      "Epoch 76/100\n",
      "303/303 [==============================] - 339s 1s/step - loss: 0.2312 - mae: 0.2312 - val_loss: 0.1875 - val_mae: 0.1875 - lr: 3.4868e-04\n",
      "Epoch 77/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2299 - mae: 0.2299 - val_loss: 0.1566 - val_mae: 0.1566 - lr: 3.4868e-04\n",
      "Epoch 78/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2268 - mae: 0.2268\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "303/303 [==============================] - 320s 1s/step - loss: 0.2268 - mae: 0.2268 - val_loss: 0.1432 - val_mae: 0.1432 - lr: 3.4868e-04\n",
      "Epoch 79/100\n",
      "303/303 [==============================] - 325s 1s/step - loss: 0.2253 - mae: 0.2253 - val_loss: 0.1199 - val_mae: 0.1199 - lr: 3.1381e-04\n",
      "Epoch 80/100\n",
      "303/303 [==============================] - 326s 1s/step - loss: 0.2163 - mae: 0.2163 - val_loss: 0.2049 - val_mae: 0.2049 - lr: 3.1381e-04\n",
      "Epoch 81/100\n",
      "303/303 [==============================] - 327s 1s/step - loss: 0.2183 - mae: 0.2183 - val_loss: 0.2341 - val_mae: 0.2341 - lr: 3.1381e-04\n",
      "Epoch 82/100\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2198 - mae: 0.2198 - val_loss: 0.1316 - val_mae: 0.1316 - lr: 3.1381e-04\n",
      "Epoch 83/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2219 - mae: 0.2219\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "303/303 [==============================] - 322s 1s/step - loss: 0.2219 - mae: 0.2219 - val_loss: 0.1410 - val_mae: 0.1410 - lr: 3.1381e-04\n",
      "Epoch 84/100\n",
      "303/303 [==============================] - 326s 1s/step - loss: 0.2128 - mae: 0.2128 - val_loss: 0.1192 - val_mae: 0.1192 - lr: 2.8243e-04\n",
      "Epoch 85/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.2191 - mae: 0.2191 - val_loss: 0.1169 - val_mae: 0.1169 - lr: 2.8243e-04\n",
      "Epoch 86/100\n",
      "303/303 [==============================] - 326s 1s/step - loss: 0.2195 - mae: 0.2195 - val_loss: 0.1346 - val_mae: 0.1346 - lr: 2.8243e-04\n",
      "Epoch 87/100\n",
      "303/303 [==============================] - 337s 1s/step - loss: 0.2186 - mae: 0.2186 - val_loss: 0.1424 - val_mae: 0.1424 - lr: 2.8243e-04\n",
      "Epoch 88/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2204 - mae: 0.2204\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "303/303 [==============================] - 325s 1s/step - loss: 0.2204 - mae: 0.2204 - val_loss: 0.1195 - val_mae: 0.1195 - lr: 2.8243e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.2183 - mae: 0.2183 - val_loss: 0.1354 - val_mae: 0.1354 - lr: 2.5419e-04\n",
      "Epoch 90/100\n",
      "303/303 [==============================] - 327s 1s/step - loss: 0.2124 - mae: 0.2124 - val_loss: 0.1103 - val_mae: 0.1103 - lr: 2.5419e-04\n",
      "Epoch 91/100\n",
      "303/303 [==============================] - 336s 1s/step - loss: 0.2096 - mae: 0.2096 - val_loss: 0.2229 - val_mae: 0.2229 - lr: 2.5419e-04\n",
      "Epoch 92/100\n",
      "303/303 [==============================] - 324s 1s/step - loss: 0.2084 - mae: 0.2084 - val_loss: 0.2084 - val_mae: 0.2084 - lr: 2.5419e-04\n",
      "Epoch 93/100\n",
      "303/303 [==============================] - 343s 1s/step - loss: 0.2108 - mae: 0.2108 - val_loss: 0.0846 - val_mae: 0.0846 - lr: 2.5419e-04\n",
      "Epoch 94/100\n",
      "303/303 [==============================] - 342s 1s/step - loss: 0.2112 - mae: 0.2112 - val_loss: 0.0866 - val_mae: 0.0866 - lr: 2.5419e-04\n",
      "Epoch 95/100\n",
      "303/303 [==============================] - 384s 1s/step - loss: 0.2114 - mae: 0.2114 - val_loss: 0.0962 - val_mae: 0.0962 - lr: 2.5419e-04\n",
      "Epoch 96/100\n",
      "303/303 [==============================] - 400s 1s/step - loss: 0.2124 - mae: 0.2124 - val_loss: 0.1431 - val_mae: 0.1431 - lr: 2.5419e-04\n",
      "Epoch 97/100\n",
      "303/303 [==============================] - 344s 1s/step - loss: 0.2036 - mae: 0.2036 - val_loss: 0.0984 - val_mae: 0.0984 - lr: 2.5419e-04\n",
      "Epoch 98/100\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.2080 - mae: 0.2080\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
      "303/303 [==============================] - 339s 1s/step - loss: 0.2080 - mae: 0.2080 - val_loss: 0.0884 - val_mae: 0.0884 - lr: 2.5419e-04\n",
      "Epoch 99/100\n",
      "303/303 [==============================] - 353s 1s/step - loss: 0.2067 - mae: 0.2067 - val_loss: 0.1577 - val_mae: 0.1577 - lr: 2.2877e-04\n",
      "Epoch 100/100\n",
      "303/303 [==============================] - 383s 1s/step - loss: 0.2034 - mae: 0.2034 - val_loss: 0.0935 - val_mae: 0.0935 - lr: 2.2877e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Anurag\\AppData\\Local\\Temp\\tmpujeefpq4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Anurag\\AppData\\Local\\Temp\\tmpujeefpq4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 31s 360ms/step\n",
      "[[18.072456 ]\n",
      " [17.027466 ]\n",
      " [ 3.9789872]\n",
      " [ 7.03244  ]\n",
      " [18.055027 ]\n",
      " [18.965885 ]\n",
      " [15.044263 ]\n",
      " [20.151802 ]\n",
      " [ 5.061651 ]\n",
      " [ 3.0553079]]\n",
      "[18. 17.  4.  7. 18. 19. 15. 20.  5.  3.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = \"ImagePro\"\n",
    "files=os.listdir(path)\n",
    "\n",
    "files.sort()\n",
    "\n",
    "print(files)\n",
    "\n",
    "image_array=[]\n",
    "label_array=[]\n",
    "for i in range(len(files)):\n",
    "    #list of image in each folder\n",
    "    sub_file= os.listdir(path+\"/\"+files[i])\n",
    "    #print(len(sub_file))\n",
    "    \n",
    "    #loop through each subfile\n",
    "    for j in tqdm(range(len(sub_file))):\n",
    "        #path of each image\n",
    "        # Example : Imagepro/A/image_name1.jpg\n",
    "        file_path=path+\"/\"+files[i]+\"/\"+sub_file[j]\n",
    "        #read each image\n",
    "        \n",
    "        image=cv2.imread(file_path)\n",
    "        \n",
    "        #resize image by 96x96\n",
    "        image = cv2.resize(image,(96,96))\n",
    "        #convert BGR image to RGB image\n",
    "        \n",
    "        image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #add this image to image_array\n",
    "        \n",
    "        image_array.append(image)\n",
    "        \n",
    "        #add label to label array\n",
    "        # i is number form 0 to len(files)-1\n",
    "        #so we can use it as label\n",
    "        label_array.append(i)\n",
    "        \n",
    "\n",
    "image_array=np.array(image_array)\n",
    "label_array=np.array(label_array,dtype=\"float\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(image_array,label_array,test_size=0.15)\n",
    "\n",
    "\n",
    "del image_array,label_array\n",
    "#X_train Will have 85% of images\n",
    "#X_test will have 15%of images\n",
    "import gc\n",
    "gc.collect()\n",
    "#Create a model\n",
    "\n",
    "from keras import layers,callbacks,utils,applications,optimizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "model= Sequential()\n",
    "# add pretrained models to sequentail model\n",
    "# i will use EfficientNetB0 pretrained model \n",
    "pretrained_model= tf.keras.applications.EfficientNetB0(input_shape=(96,96,3),include_top=False)\n",
    "model.add(pretrained_model)\n",
    "\n",
    "#add Pooling to model\n",
    "\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "# add dropout to model\n",
    "#we add droupout to increase accracy by reduce overfitting\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "#finally we will add dense layer to output\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# For some tensorflow version we required to build model\n",
    "model.build(input_shape=(None,96,96,3))\n",
    "\n",
    "#to see model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#save and run \n",
    "\n",
    "model.compile(optimizer=\"adam\", loss = \"mae\", metrics =[\"mae\"])\n",
    "\n",
    "# create a checkpoint to save best acuracy model\n",
    "\n",
    "ckp_path=\"trained_model/model\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=ckp_path,monitor=\"val_mae\", save_best_only=True, save_weights_only=True)\n",
    "\n",
    "#monitor: monitor validation mae loss to save model\n",
    "#mode ; use to save model  when it is minimum or maximum\n",
    "# It has 3 option: \"min\",\"max\", \"auto\"\n",
    "# for us you can select either \"min\" or \"auto\"\n",
    "\n",
    "#when val_mae reduce model will be saved\n",
    "# save_best_only: False -> It will save all model\n",
    "#save_weight_olny: Save only weight.\n",
    "\n",
    "#create learning rate reducer to reduce lr when accuracy does not import\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9,monitor=\"val_mae\",\n",
    "                                         mode=\"auto\", cooldown = 0,\n",
    "                                        patience = 5,\n",
    "                                        verbose =1, min_lr=1e-6)\n",
    "\n",
    "# factor: when it is reduce next lr will be 0.9 times of current\n",
    "# next lr = 0.9*current lr\n",
    "\n",
    "# patience= X\n",
    "# reduce lr after x epoch when accuracy does not imporve\n",
    "# verbose: show it after ever epoch\n",
    "\n",
    "#min_lr: minimum learning rate\n",
    "\n",
    "# Start training model\n",
    "\n",
    "Epoch = 100\n",
    "Batch_Size= 32\n",
    "\n",
    "# select batch size accordingly to your Graph card \n",
    "# \n",
    "\n",
    "history = model.fit(\n",
    "                    X_train,\n",
    "                    Y_train,\n",
    "                    validation_data = (X_test, Y_test),\n",
    "                    batch_size = Batch_Size,\n",
    "                    epochs = Epoch,\n",
    "                    callbacks = [model_checkpoint,reduce_lr])\n",
    "\n",
    "model.load_weights(ckp_path)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model=converter.convert()\n",
    "\n",
    "with open(\"model.tflite\",\"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "    \n",
    "prediction_val=model.predict(X_test, batch_size = 32)\n",
    "\n",
    "print(prediction_val[:10])\n",
    "print(Y_test[:10])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
